{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLecX4XsEoMH"
      },
      "source": [
        "# Getting Started with Metrax üöÄ\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google/metrax/blob/main/metrax_example.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Welcome to this hands-on guide for `metrax`, a powerful and flexible metrics library for JAX.\n",
        "\n",
        "In this Colab, you'll learn how to:\n",
        "* Use the **Functional metrax API** (`metrax`) and the **Object-Oriented metrax API** (`metrax.nnx`).\n",
        "* Verify that batch and iterative calculations give **identical results**.\n",
        "* Scale your metric computations to **multiple devices** using 1)`jax.pmap` and 2)`jax.jit`.\n",
        "* Scale your metric computations to **multiple hosts** for large scale distributed training, using 1) `Multi-Controller JAX` and 2) `Cloud Pathways` solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTwMjngoy-h8"
      },
      "source": [
        "## ‚öôÔ∏è Environment Setup: Simulating Multiple Devices\n",
        "\n",
        "First, let's configure our environment. To demonstrate `metrax`'s multi-device capabilities, we'll instruct JAX to simulate an environment with **4 virtual CPU devices**. This allows us to test `jax.pmap` and `jax.jit` with `mesh` logic even on single-device hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-eEZKa8qHy7"
      },
      "outputs": [],
      "source": [
        "!pip install google-metrax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhooimvEyzFc"
      },
      "outputs": [],
      "source": [
        "# This environment variable instructs JAX's underlying XLA compiler\n",
        "# to create a specific number of virtual CPU devices.\n",
        "#\n",
        "# This MUST be set *before* the JAX backend is initialized, which happens on\n",
        "# the first import of `jax`.\n",
        "#\n",
        "# In a script, ensure this line comes before `import jax`.\n",
        "# In a notebook, a kernel restart may be needed if JAX has already been used.\n",
        "import os\n",
        "print(\"Configuring JAX to simulate 4 CPU devices...\")\n",
        "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=4'\n",
        "import jax\n",
        "\n",
        "\n",
        "# --- Verify the JAX Environment ---\n",
        "print(\"\\nVerifying JAX environment configuration:\")\n",
        "print(\"-\" * 40)\n",
        "device_count = jax.device_count()\n",
        "process_count = jax.process_count()\n",
        "print(f\"‚úÖ Number of available JAX devices: {device_count}\")\n",
        "print(f\"‚úÖ Number of JAX processes: {process_count}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if device_count == 4:\n",
        "  print(\"Success! JAX is now set up for multi-device simulation.\")\n",
        "else:\n",
        "  print(\"Warning: JAX device count is not the expected value.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjjUn5OVwhQW"
      },
      "source": [
        "## üìä Data Preparation for Realistic Scenarios\n",
        "\n",
        "Next, let's generate some data. A good metrics demo uses realistic data, so we'll create a dataset that is **imbalanced** and where the model's **predictions are imperfect but correlated** with the true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppl-mTUk6XWC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. Data Generation Setup ---\n",
        "np.random.seed(42)\n",
        "N_BATCHES = 4\n",
        "BATCH_SIZE = 8\n",
        "TOTAL_SAMPLES = N_BATCHES * BATCH_SIZE\n",
        "\n",
        "# --- 2. Create Realistic, Correlated Data ---\n",
        "# Create an imbalanced dataset (80% class 0, 20% class 1).\n",
        "labels = np.random.choice([0, 1], size=(TOTAL_SAMPLES,), p=[0.8, 0.2])\n",
        "\n",
        "# Generate predictions correlated with labels, adding some noise for realism.\n",
        "noise = np.random.normal(loc=0, scale=0.25, size=TOTAL_SAMPLES)\n",
        "clean_preds = np.where(labels == 1, 0.8, 0.2)\n",
        "predictions = np.clip(clean_preds + noise, 0, 1)\n",
        "\n",
        "# Generate sample weights to give more importance to the rare positive class.\n",
        "sample_weights = np.where(labels == 1, 2.0, 1.0)\n",
        "\n",
        "# --- 3. Reshape Data into Batched Format ---\n",
        "# The batched format is useful for demonstrating iterative calculations.\n",
        "labels_batched = labels.reshape(N_BATCHES, BATCH_SIZE).astype(np.float32)\n",
        "predictions_batched = predictions.reshape(N_BATCHES, BATCH_SIZE).astype(np.float32)\n",
        "sample_weights_batched = sample_weights.reshape(N_BATCHES, BATCH_SIZE).astype(np.float32)\n",
        "\n",
        "# --- 4. Data Shape Verification ---\n",
        "print(\"‚úÖ Data generation complete. Verifying array shapes:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Flat arrays for full-dataset processing:\")\n",
        "print(f\"  - predictions.shape:    {predictions.shape}\")\n",
        "print(f\"  - labels.shape:         {labels.shape}\")\n",
        "print(f\"\\nBatched arrays for iterative/streaming processing:\")\n",
        "print(f\"  - predictions_batched.shape: {predictions_batched.shape}\")\n",
        "print(f\"  - labels_batched.shape:      {labels_batched.shape}\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZiutTU3qguP"
      },
      "source": [
        "## Lifecycle of a Metrax Metric\n",
        "\n",
        "The typical lifecycle of a `metrax` metric involves a few steps, especially when processing data in batches or across distributed environments:\n",
        "\n",
        "1. **Initialization:** You usually start by creating a dictionary where keys represent the metric names and values are the initialized or uninitialized `metrax` metric states. When using the functional API (`metrax`), you'd use `Metric.empty()` to create an initial, empty state. With the object-oriented `metrax.nnx` API, you simply instantiate the metric class (`Metric()`).\n",
        "2. **Iteration/Batch Processing:** As you process data in batches or on different devices, you create a new metric state for the current batch/device using `Metric.from_model_output()` (functional API) or update the existing metric object with `.update()` (object-oriented API), passing the predictions, labels, and any relevant weights for that specific data slice.\n",
        "3. **Merging/Updating:** For the functional API, you merge the newly created metric state for the current batch/device with the accumulated state in your dictionary using the `.merge()` method. For the object-oriented API, the `.update()` method directly modifies the state within the metric object in your dictionary. This step accumulates the necessary statistics across all processed data.\n",
        "4. **Final Computation:** After processing all data, you call the `.compute()` method on the final, merged (functional API) or updated (object-oriented API) metric state in your dictionary. This performs the final calculations and returns the metric's value (e.g., a single floating-point number like AUCPR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4E5A--_cF86"
      },
      "source": [
        "## Metrax Linen API (Functional)\n",
        "\n",
        "The core `metrax` API is functional and stateless, making it a natural fit for JAX. It works by creating immutable `Metric` state objects that can be merged.\n",
        "\n",
        "Each `metrax` metric inherits the CLU [`metric`](http://shortn/_e70RtO7j36) class and provides the following APIs:\n",
        "\n",
        "* `Metric.from_model_output()`: Creates a metric state from data.\n",
        "* `Metric.empty()`: Creates an empty, initial state.\n",
        "* `metric_a.merge(metric_b)`: Combines two metric states.\n",
        "* `metric.compute()`: Computes the final value.\n",
        "\n",
        "Let's demonstrate by calculating several metrics on our dataset, once on the 1) full batch and once by 2) iteratively merging results. The second method resembles real world machine learning metrics calculation scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOT_PXRDv80K"
      },
      "source": [
        "###Basic Usage: Unweighted Metrics\n",
        "This first example demonstrates the core functional workflow without sample weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFHRQRmpxmys"
      },
      "outputs": [],
      "source": [
        "import metrax\n",
        "\n",
        "# Define the metrics we want to calculate.\n",
        "metrics_to_compute = {\n",
        "    'Precision': metrax.Precision,\n",
        "    'Recall': metrax.Recall,\n",
        "    'AUCPR': metrax.AUCPR,\n",
        "    'AUCROC': metrax.AUCROC,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eexpYXIELRUd"
      },
      "outputs": [],
      "source": [
        "# --- Method 1: Full-Batch Calculation (Unweighted) ---\n",
        "print(\"--- Method 1: Full-Batch Calculation (Unweighted) ---\")\n",
        "full_batch_results = {}\n",
        "for name, MetricClass in metrics_to_compute.items():\n",
        "  metric_state = MetricClass.from_model_output(\n",
        "      predictions=predictions,\n",
        "      labels=labels\n",
        "  )\n",
        "  full_batch_results[name] = metric_state.compute()\n",
        "  print(f\"{name}: {full_batch_results[name]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYZe0JUQxiNt"
      },
      "outputs": [],
      "source": [
        "# --- Method 2: Iterative Merging by Batch (Unweighted) ---\n",
        "print(\"\\n--- Method 2: Iterative Merging (Unweighted) ---\")\n",
        "iterative_metrics = {\n",
        "    name: MetricClass.empty() for name, MetricClass in metrics_to_compute.items()\n",
        "}\n",
        "\n",
        "for labels_b, predictions_b in zip(labels_batched, predictions_batched):\n",
        "  for name, MetricClass in metrics_to_compute.items():\n",
        "    current_metric_state = MetricClass.from_model_output(\n",
        "        predictions=predictions_b,\n",
        "        labels=labels_b\n",
        "    )\n",
        "    iterative_metrics[name] = iterative_metrics[name].merge(current_metric_state)\n",
        "\n",
        "iterative_results = {}\n",
        "for name, metric_state in iterative_metrics.items():\n",
        "  iterative_results[name] = metric_state.compute()\n",
        "  print(f\"{name}: {iterative_results[name]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_HJqaJKv_73"
      },
      "source": [
        "###Advanced Usage: Incorporating Sample Weights\n",
        "Just like the NNX API, the functional API supports sample_weights in from_model_output for metrics where it is applicable. The following example calculates AUCPR and AUCROC using the weighted data we prepared earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czPiI6dhxvmL"
      },
      "outputs": [],
      "source": [
        "import metrax\n",
        "\n",
        "# Define which of these metrics should receive sample weights.\n",
        "metrics_with_weights = {\n",
        "    'AUCPR': metrax.AUCPR,\n",
        "    'AUCROC': metrax.AUCROC,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSnndNonwDKq"
      },
      "outputs": [],
      "source": [
        "# --- Method 1: Full-Batch Calculation (Weighted) ---\n",
        "print(\"--- Method 1: Full-Batch Calculation (Weighted) ---\")\n",
        "full_batch_results_weighted = {}\n",
        "for name, MetricClass in metrics_with_weights.items():\n",
        "  metric_state = MetricClass.from_model_output(\n",
        "      predictions=predictions,\n",
        "      labels=labels,\n",
        "      sample_weights=sample_weights\n",
        "  )\n",
        "  full_batch_results_weighted[name] = metric_state.compute()\n",
        "  print(f\"{name}: {full_batch_results_weighted[name]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8BPLW6XxxVr"
      },
      "outputs": [],
      "source": [
        "# --- Method 2: Iterative Merging by Batch (Weighted) ---\n",
        "print(\"\\n--- Method 2: Iterative Merging (Weighted) ---\")\n",
        "iterative_metrics_weighted = {\n",
        "    name: MetricClass.empty() for name, MetricClass in metrics_with_weights.items()\n",
        "}\n",
        "for labels_b, predictions_b, weights_b in zip(labels_batched, predictions_batched, sample_weights_batched):\n",
        "  for name, MetricClass in metrics_with_weights.items():\n",
        "    current_metric_state = MetricClass.from_model_output(\n",
        "        predictions=predictions_b,\n",
        "        labels=labels_b,\n",
        "        sample_weights=weights_b\n",
        "    )\n",
        "    iterative_metrics_weighted[name] = iterative_metrics_weighted[name].merge(current_metric_state)\n",
        "\n",
        "iterative_results_weighted = {}\n",
        "for name, metric_state in iterative_metrics_weighted.items():\n",
        "  iterative_results_weighted[name] = metric_state.compute()\n",
        "  print(f\"{name}: {iterative_results_weighted[name]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCez70DmcIEr"
      },
      "source": [
        "## Metrax NNX API (Object-Oriented)\n",
        "\n",
        "For users who prefer an object-oriented style, `metrax.nnx` provides stateful metric objects. This can simplify the code for iterative updates, as you update a single object in place.\n",
        "\n",
        "Each `metrax.nnx` metric inherits the NNX [`metric`](http://shortn/_VyVVvvsQ00) class and provides the following APIs:\n",
        "\n",
        "* `metric = Metric()`: Creates a stateful metric object.\n",
        "* `metric.update()`: Updates the metric's internal state with new data.\n",
        "* `metric.compute()`: Computes the final value from the accumulated state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TokxcQvTvWua"
      },
      "source": [
        "###Basic Usage: Unweighted Metrics\n",
        "Let's start with the simplest use case: calculating metrics without any sample weights. This example demonstrates the core object-oriented workflow. Note that for Precision and Recall, metrax uses a default classification threshold of 0.5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0ULds_Sx06D"
      },
      "outputs": [],
      "source": [
        "import metrax.nnx\n",
        "\n",
        "# Define the nnx metrics we want to calculate.\n",
        "metrics_to_compute_nnx = {\n",
        "    'Precision': metrax.nnx.Precision,\n",
        "    'Recall': metrax.nnx.Recall,\n",
        "    'AUCPR': metrax.nnx.AUCPR,\n",
        "    'AUCROC': metrax.nnx.AUCROC,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4WucRk0O2rg"
      },
      "outputs": [],
      "source": [
        "# --- Method 1: Full-Batch Calculation (nnx) ---\n",
        "print(\"--- Method 1: Full-Batch Calculation with nnx (Unweighted) ---\")\n",
        "full_batch_metrics_nnx = {\n",
        "    name: MetricClass() for name, MetricClass in metrics_to_compute_nnx.items()\n",
        "}\n",
        "\n",
        "for name, metric_obj in full_batch_metrics_nnx.items():\n",
        "  # Basic update with just predictions and labels\n",
        "  metric_obj.update(predictions=predictions, labels=labels)\n",
        "\n",
        "full_batch_results_nnx = {}\n",
        "for name, metric_obj in full_batch_metrics_nnx.items():\n",
        "  full_batch_results_nnx[name] = metric_obj.compute()\n",
        "  print(f\"{name}: {full_batch_results_nnx[name]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hz14HRfx5vM"
      },
      "outputs": [],
      "source": [
        "# --- Method 2: Iterative Updating by Batch (nnx) ---\n",
        "print(\"\\n--- Method 2: Iterative Updating with nnx (Unweighted) ---\")\n",
        "iterative_metrics_nnx = {\n",
        "    name: MetricClass() for name, MetricClass in metrics_to_compute_nnx.items()\n",
        "}\n",
        "\n",
        "for labels_b, predictions_b, _ in zip(labels_batched, predictions_batched, sample_weights_batched):\n",
        "  for name, metric_obj in iterative_metrics_nnx.items():\n",
        "    metric_obj.update(predictions=predictions_b, labels=labels_b)\n",
        "\n",
        "iterative_results_nnx = {}\n",
        "for name, metric_obj in iterative_metrics_nnx.items():\n",
        "  iterative_results_nnx[name] = metric_obj.compute()\n",
        "  print(f\"{name}: {iterative_results_nnx[name]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhq1dM5Rvq-Q"
      },
      "source": [
        "###Advanced Usage: Incorporating Sample Weights\n",
        "In many real-world scenarios, you'll want to assign different importance to different examples. This is often done to handle class imbalance, where you might give more weight to examples from a rare class. metrax.nnx supports this through the sample_weights argument in the .update() method.\n",
        "\n",
        "The following example calculates AUCPR and AUCROC, which are metrics that support sample weights, using the weighted data we prepared earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB2z33Jax8md"
      },
      "outputs": [],
      "source": [
        "import metrax.nnx\n",
        "\n",
        "# Define which metrics should receive sample weights.\n",
        "weighted_metrics_to_compute_nnx = {\n",
        "    'AUCPR': metrax.nnx.AUCPR,\n",
        "    'AUCROC': metrax.nnx.AUCROC,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HERtwSZbvs6-"
      },
      "outputs": [],
      "source": [
        "# --- Method 1: Full-Batch Calculation with Sample Weights ---\n",
        "print(\"--- Method 1: Full-Batch Calculation with nnx (Weighted) ---\")\n",
        "full_batch_metrics_weighted = {\n",
        "    name: MetricClass() for name, MetricClass in weighted_metrics_to_compute_nnx.items()\n",
        "}\n",
        "\n",
        "for name, metric_obj in full_batch_metrics_weighted.items():\n",
        "  # Update with predictions, labels, AND sample_weights\n",
        "  metric_obj.update(\n",
        "      predictions=predictions,\n",
        "      labels=labels,\n",
        "      sample_weights=sample_weights\n",
        "  )\n",
        "\n",
        "full_batch_results_weighted = {}\n",
        "for name, metric_obj in full_batch_metrics_weighted.items():\n",
        "  full_batch_results_weighted[name] = metric_obj.compute()\n",
        "  print(f\"{name}: {full_batch_results_weighted[name]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTAmdmyHx-wi"
      },
      "outputs": [],
      "source": [
        "# --- Method 2: Iterative Updating with Sample Weights ---\n",
        "print(\"\\n--- Method 2: Iterative Updating with nnx (Weighted) ---\")\n",
        "iterative_metrics_weighted = {\n",
        "    name: MetricClass() for name, MetricClass in weighted_metrics_to_compute_nnx.items()\n",
        "}\n",
        "\n",
        "for labels_b, predictions_b, weights_b in zip(labels_batched, predictions_batched, sample_weights_batched):\n",
        "  for name, metric_obj in iterative_metrics_weighted.items():\n",
        "    metric_obj.update(\n",
        "        predictions=predictions_b,\n",
        "        labels=labels_b,\n",
        "        sample_weights=weights_b\n",
        "    )\n",
        "\n",
        "iterative_results_weighted = {}\n",
        "for name, metric_obj in iterative_metrics_weighted.items():\n",
        "  iterative_results_weighted[name] = metric_obj.compute()\n",
        "  print(f\"{name}: {iterative_results_weighted[name]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_MHjDaRzwBf"
      },
      "source": [
        "## Scaling to Multiple Devices\n",
        "\n",
        "`metrax` is designed from the ground up to work seamlessly in distributed environments. `Metrax` metrics are compatible with JAX's core parallelism primitives. The standard and most flexible way to scale computations is by using `jax.jit` in combination with the `jax.sharding` API to create explicitly sharded, Single-Program, Multiple-Data (SPMD) programs.\n",
        "\n",
        "### The `jit` and `Mesh` Approach (Explicit Sharding)\n",
        "\n",
        "For maximum flexibility and control over distributed computation, JAX provides an **explicit** sharding mechanism using the `jax.sharding` and `Mesh` APIs. This approach is more powerful than older APIs and is the standard for large-scale models that may require complex parallelism strategies (like model parallelism).\n",
        "\n",
        "The process involves a few clear steps:\n",
        "\n",
        "1.  **Define a `Mesh`**: You first create a logical grid of your physical devices and give names to the axes (e.g., `Mesh(jax.devices(), ('data',))`). This describes the topology you'll be working with.\n",
        "2.  **Create a `Sharding` Rule**: You specify exactly how each dimension of your array should be mapped to the mesh's axes. This is done using `NamedSharding` and `PartitionSpec`. For data parallelism, you would shard the batch axis of your data across the `'data'` axis of your mesh.\n",
        "3.  **Explicitly Place Data**: You use `jax.device_put` to apply this sharding rule to your data arrays. At this point, your JAX arrays are \"aware\" of how they are distributed across the physical hardware.\n",
        "4.  **`jit`-Compile the Function**: You write a function that looks like a normal, single-device calculation and decorate it with `@jax.jit`. When JAX's compiler sees that the inputs to this function are sharded arrays, it automatically generates a distributed version of the code, implicitly handling all cross-device communication.\n",
        "\n",
        "This method provides the fine-grained control that is essential for all modern JAX parallelism patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkVcKJRPAbRE"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import numpy as np\n",
        "import metrax\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
        "\n",
        "# This script assumes that the JAX environment is configured for 4 devices\n",
        "# and that the data arrays `predictions`, `labels`, and `sample_weights`\n",
        "# have been created in a previous cell.\n",
        "\n",
        "# Baseline: Single-Device (Direct)\n",
        "@jax.jit\n",
        "def calculate_aucpr_direct(predictions, labels, sample_weights):\n",
        "  \"\"\"Computes AUCPR on the entire dataset on a single device.\"\"\"\n",
        "  return metrax.AUCPR.from_model_output(\n",
        "      predictions=predictions,\n",
        "      labels=labels,\n",
        "      sample_weights=sample_weights\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGsGxzR3Afsv"
      },
      "outputs": [],
      "source": [
        "# Advanced SPMD Parallelism: jit + Mesh\n",
        "def calculate_aucpr_mesh(predictions, labels, sample_weights):\n",
        "    \"\"\"\n",
        "    Explicitly shards data across a device Mesh and calculates with jit.\n",
        "    \"\"\"\n",
        "    # 1. Define the device mesh and sharding rule.\n",
        "    mesh = Mesh(jax.devices(), axis_names=('data',))\n",
        "    sharding_rule = NamedSharding(mesh, PartitionSpec('data'))\n",
        "\n",
        "    # 2. Explicitly move and shard the data onto the mesh.\n",
        "    sharded_predictions = jax.device_put(predictions, sharding_rule)\n",
        "    sharded_labels = jax.device_put(labels, sharding_rule)\n",
        "    sharded_weights = jax.device_put(sample_weights, sharding_rule)\n",
        "\n",
        "    # 3. Define the function to be JIT-compiled.\n",
        "    def _calculate(preds, labs, weights):\n",
        "      return metrax.AUCPR.from_model_output(\n",
        "          predictions=preds, labels=labs, sample_weights=weights)\n",
        "\n",
        "    # 4. JIT-compile the function with explicit sharding annotations.\n",
        "    #    - in_shardings: Specifies how each input array is expected to be sharded.\n",
        "    #    - out_sharding: Specifies the desired sharding for the output.\n",
        "    #                    'None' means the output should be replicated on all devices.\n",
        "    jitted_calculate = jax.jit(\n",
        "        _calculate,\n",
        "        in_shardings=(sharding_rule, sharding_rule, sharding_rule),\n",
        "        out_shardings=None\n",
        "    )\n",
        "\n",
        "    # The result is already a globally correct metric state, replicated on all devices.\n",
        "    return jitted_calculate(sharded_predictions, sharded_labels, sharded_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3r37x3sMQbx"
      },
      "outputs": [],
      "source": [
        "print(\"\\nRunning all three AUCPR calculation methods...\")\n",
        "\n",
        "# Execute each of the three methods.\n",
        "state_pmap = calculate_aucpr_pmap(predictions, labels, sample_weights)\n",
        "state_mesh = calculate_aucpr_mesh(predictions, labels, sample_weights)\n",
        "state_direct = calculate_aucpr_direct(predictions, labels, sample_weights)\n",
        "\n",
        "# Compute the final values from the metric states.\n",
        "result_pmap = state_pmap.compute()\n",
        "result_mesh = state_mesh.compute()\n",
        "result_direct = state_direct.compute()\n",
        "\n",
        "# Ensure all computations are finished before verifying.\n",
        "result_pmap.block_until_ready()\n",
        "result_mesh.block_until_ready()\n",
        "result_direct.block_until_ready()\n",
        "\n",
        "# Verify that all results are numerically identical.\n",
        "assert np.allclose(result_pmap, result_direct, rtol=1e-6)\n",
        "assert np.allclose(result_mesh, result_direct, rtol=1e-6)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"          Comparison of Multi-Device AUCPR Calculations\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Method':<35} {'AUCPR Value'}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Method 1: pmap':<35} {result_pmap}\")\n",
        "print(f\"{'Method 2: jit + Mesh':<35} {result_mesh}\")\n",
        "print(f\"{'Baseline: Direct Single-Device':<35} {result_direct}\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚úÖ Verification successful: All three methods yield identical results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3YWS1_x19DJ"
      },
      "source": [
        "## üß† Advanced Use: Multi-Host Environments\n",
        "\n",
        "For large-scale training (e.g., on TPU Pods), JAX uses multiple hosts (controllers), each managing multiple devices. In these scenarios, `metrax` integrates seamlessly with JAX's sharding capabilities provided by `jax.sharding` and `Mesh`.\n",
        "\n",
        "By `jit`-compiling your training step with the appropriate device mesh, you can calculate metrics across hundreds or thousands of devices without changing the core metric logic.\n",
        "\n",
        "We will soon provide a detailed example of multi-controller and Cloud Pathways training."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
